= Cloud SDK pass:[<br/>] サンプルアプリケーション pass:[<br/>] Python版 pass:[<br/>] チュートリアル pass:[<br/>] 
:sectnums:
:sectnumlevels: 1
:author: Copyright 2023 Sony Semiconductor Solutions Corporation
:version-label: Version 
:revnumber: x.x.x
:revdate: YYYY - MM - DD
:trademark-desc: AITRIOS™、およびそのロゴは、ソニーグループ株式会社またはその関連会社の登録商標または商標です。
:toc:
:toc-title: 目次
:toclevels: 1
:chapter-label:
:lang: ja

== 更新履歴

|===
|Date |What/Why 

|2022/12/12
|初版作成

|2023/1/30
|誤記修正 + 
表現統一 + 
記法修正 + 
図文字サイズ修正 + 
PDFビルド環境更新 +
サンプルアプリケーションリポジトリ構成から[console_access_settings.yaml]を削除 +
「サンプルアプリケーション実行の準備をする」の接続先情報設定方法変更 +
ユースケース毎の実装説明の「**Cloud SDK**」の0.2.0対応

|2023/5/26
|FlatBuffers version upにともなう「サンプルアプリケーションリポジトリ構成」「参考資料」修正 +
「サンプルアプリケーションが依存するPackage（フレームワーク）」にFlatBuffers不足していたため追加 +
「ユースケース毎の実装説明」のコード引用ミス修正 + 
ツール名の括弧の表記の修正 + 
図の代替テキスト追加

|2023/12/22
|Console Developer EditionとConsole Enterprise Edition対応

|===

== はじめに
このチュートリアルでは、「**Cloud SDK**」を用いたサンプルアプリケーションについて解説します。 +
このサンプルアプリケーションは、「**Cloud SDK**」の基本的な使い方を体験して頂くために用意しています。 +
サンプルアプリケーションでは、「**Cloud SDK**」を利用してエッジAIデバイスを制御する方法と、「**Console for AITRIOS**」 (以下、「**Console**」と記載)もしくは「Azure Blob Storage」もしくは「Local Storage」UploadされたエッジAIデバイスの出力を確認する方法を確認できます。 +
ただし、「Local Storage」へアップロードされた推論結果を確認する場合、Codespacesは利用できません。

[#_precondition]
== 前提条件
=== 接続情報
サンプルアプリケーションを使用するには、アプリケーションから「**Console**」へアクセスするための接続情報が必要になります。 +
取得した情報は<<#_Execute_sampleapp,「1.サンプルアプリケーション実行の準備をする」>>で利用します。 +
必要な接続情報は下記の通りです。

* クライアントアプリケーション詳細情報
- 「**Console Developer Edition**」を使用している場合
** 「**Portal for AITRIOS**」のクライアントアプリケーション一覧から参照または、必要に応じてサンプルアプリケーション向けのクライアントアプリケーション登録を行い、下記情報の取得を行ってください。
詳細は、 https://developer.aitrios.sony-semicon.com/documents/portal-user-manual[「**Portalユーザーマニュアル**」] の「SDK用のClient Secretを発行する」をお読みください。
*** クライアントID
*** シークレット
+
** https://developer.aitrios.sony-semicon.com/file/download/rest-api-authentication[こちらのドキュメント] から下記の情報を取得してください。
*** コンソールエンドポイント
*** ポータル認証エンドポイント

- 「**Console Enterprise Edition**」を使用している場合
** 「**Console**」の導入担当者(Service Administrator)に問い合わせください。


「Azure Blob Storage」を使用するには、「Azure Blob Storage」へアクセスするための接続情報が必要になります。
詳細は、 https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string#configure-a-connection-string-for-an-azure-storage-account[ 「**Configure a connection string for an Azure storage account**」 ]をお読みください。

=== エッジAIデバイス
サンプルアプリケーションを正常に動作させるためには、利用するエッジAIデバイスに特定の設定が必要になります。 +
必要な設定内容は下記の通りです。

* AIモデルやアプリケーションがデプロイされていること
* AIモデルに、Object DetectionのAIモデルがデプロイされていること
* 「**Console**」のUIから、利用するCommand Parameter Fileを下記の設定にしておくこと
** 「**Console**」利用時 +
記述しなかった場合は以下の値が自動で設定されます。 +
UploadMethod="BlobStorage" +
UploadMethodIR="Mqtt"
** 「Azure Blob Storage」 +
UploadMethod="BlobStorage" +
UploadMethodIR="BlobStorage"
** 「Local Storage」 +
UploadMethod="HTTPStorage" +
UploadMethodIR="HTTPStorage" 
+
** AIモデルやアプリケーションの内容に応じて、その他のパラメータも変更する必要がある

=== 外部転送設定
* 「Azure Blob Storage」利用時 +
「Azure Blob Storage」を使用する場合は、 https://developer.aitrios.sony-semicon.com/edge-ai-sensing/documents/external-transfer-settings-tutorial-for-azure-blob-storage[外部転送設定チュートリアル(Azure Blob Storage)]の設定を完了させてください。
* 「Local Storage」利用時 +
「Local Storage」を使用する場合は、 https://developer.aitrios.sony-semicon.com/edge-ai-sensing/documents/external-transfer-settings-tutorial-for-http-server[外部転送設定チュートリアル(Local HTTP Server)]の設定を完了させてください。
+
IMPORTANT: デバイスからHTTP ServerへのアップロードはHTTP通信のため暗号化されません。

== サンプルアプリケーション動作環境
https://developer.aitrios.sony-semicon.com/downloads#sdk-getting-started[「**SDK スタートガイド**」]を参照してください。

== サンプルアプリケーション機能概要
サンプルアプリケーションでは、「**Console**」に登録されたエッジAIデバイスを指定し、アプリケーションが推論結果と画像を取得するために必要な機能を実装しています。 +
実装されている機能は下記の三点です。

* 「**Console**」に登録されたエッジAIデバイスの情報取得
* エッジAIデバイスへの推論開始・停止指示
+
推論開始を行うことによって、エッジAIデバイスは推論結果・画像を「**Console**」もしくは「Azure Blob Storage」もしくは「Local Storage」へUploadします。
* 推論結果・画像の取得、取得結果の表示
+
「**Console**」もしくは「Azure Blob Storage」もしくは「Local Storage」にアップロードされたデータを表示します。


== サンプルアプリケーションリポジトリ構成
サンプルアプリケーションの動作環境は下記の通りです。 +
実装にかかわらない部分に関しては省略します。
----
aitrios-sdk-cloud-app-sample-python
├── src (1)
│   ├── __init__.py
│   ├── app.py  (2)
│   ├── common
│   │   ├── __init__.py
│   │   ├── deserialize  (3)
│   │   │   ├── __init__.py
│   │   │   ├── BoundingBox.py
│   │   │   ├── BoundingBox2d.py
│   │   │   ├── GeneralObject.py
│   │   │   ├── ObjectDetectionData.py
│   │   │   └── ObjectDetectionTop.py
│   │   ├── storage
│   │   │   ├── __init__.py
│   │   │   ├── get_azure_storage.py (4)
│   │   │   ├── get_console_storage.py (5)
│   │   │   ├── get_local_storage.py (6)
│   │   ├── get_client.py  (7)
│   │   └── get_deserialize_data.py  (8)
│   │   └── get_storage_data.py  (9)
│   ├── static
│   │   ├── css
│   │   │   ├── Home.module.css  (10)
│   │   │   └── reset.css  (11)
│   │   └── js
│   │       ├── label.json  (12)
│   │       └── sample.js  (13)
│   └── templates
│       └── index.html  (14)
----
(1) src : サンプルアプリケーション格納フォルダ +
(2) app.py : flask applicationのmain処理。各種REST APIで機能を実装 +
(3) deserialize : Deserialize用ソースコードを格納したフォルダ +
(4) get_azure_storage.py : 「Azure Blob Storage」から推論結果や画像を取得するロジック +
(5) get_console_storage.py : 「**Console**」から推論結果や画像を取得するロジック +
(6) get_local_storage.py : 「Local Storage」から推論結果や画像を取得するロジック +
(7) get_client.py : 「**Console Access Library**」のclientを生成 +
「Azure Blob Storage」への接続情報取得ロジック +
「Local Storage」のパス指定を行う +
(8) get_deserialize_data.py : 推論結果をDeserializeするソースコード +
(9) get_storage_data.py : 利用するStorageを判定し該当Storageのロジックを呼び出す +
(10) Home.module.css : サンプルアプリケーションのフロントエンドスタイルシート +
(11) reset.css : サンプルアプリケーションのフロントエンドスタイルシート +
(12) label.json : 推論結果の表示ラベル +
(13) sample.js : サンプルアプリケーションのフロントエンドUIで動作するJavaScriptロジック +
(14) index.html : サンプルアプリケーションのフロントエンドUI

=== ソースコードの解説

サンプルアプリケーションの概要は下記の図のようになります。

image::diagram_python.png[alt="サンプルアプリケーションの概要",width="400",align="center"]

サンプルアプリケーションはFlaskフレームワークで構成しています。

サンプルアプリケーションから「**Cloud SDK**」を呼び出し、「**Console**」を経由してエッジAIデバイスを制御します。 +
エッジAIデバイスが取得したデータは「**Console**」もしくは「Azure Blob Storage」もしくは「Local Storage」に保存されます。 +
サンプルアプリケーションは「**Cloud SDK**」等を使用して「**Console**」もしくは「Azure Blob Storage」もしくは「Local Storage」からデータを取得します。

=== サンプルアプリケーションが依存するPackage（フレームワーク）

* 「**Console Access Library**」
* https://flask.palletsprojects.com/en/2.2.x/[Flask]
* https://google.github.io/flatbuffers/[FlatBuffers]
* https://pypi.org/project/pytz/[pytz]
* https://pypi.org/project/azure-storage-blob/[azure-storage-blob]

[#_Execute_sampleapp]
== サンプルアプリケーション実行方法
<<#_precondition,前提条件>>で用意した接続情報を使用します。

=== 1.サンプルアプリケーション実行の準備をする
. Codespaces上または、リポジトリをCloneした環境上で[src/common]配下に[console_access_settings.yaml]を作成し接続先情報を設定します。

- 「**Console Developer Edition**」を使用している場合
+
|===
|src/common/console_access_settings.yaml
a|
[source,Yaml]
----
console_access_settings:
  console_endpoint: "コンソールエンドポイント"
  portal_authorization_endpoint: "ポータル認証エンドポイント"
  client_secret: "シークレット"
  client_id: "クライアントID"
----
|===
+
* `**console_endpoint**` に、取得したコンソールエンドポイントを指定します。 +
* `**portal_authorization_endpoint**` に、取得したポータル認証エンドポイントを指定します。 +
* `**client_secret**` に、登録したアプリケーションの シークレット を指定します。 +
* `**client_id**` に、登録したアプリケーションの クライアントID を指定します。 +
+

IMPORTANT: クライアントIDとシークレットの取得方法詳細は、 https://developer.aitrios.sony-semicon.com/documents/portal-user-manual[「**Portalユーザーマニュアル**」] の「SDK用のClient Secretを発行する」をお読みください。 + 
コンソールエンドポイントとポータル認証エンドポイントの取得方法詳細は、link:++https://developer.aitrios.sony-semicon.com/file/download/rest-api-authentication++[こちらのドキュメント] をお読みください。 +
これらは「**Console**」へのアクセス情報となります。 + 
公開したり、他者との共有をせず、取り扱いには十分注意してください。
+
NOTE: Proxy環境でサンプルアプリケーション実行する場合、環境変数 `**https_proxy**` の設定をしてください。

- 「**Console Enterprise Edition**」を使用している場合
+
|===
|src/common/console_access_settings.yaml
a|
[source,Yaml]
----
console_access_settings:
  console_endpoint: "コンソールエンドポイント"
  portal_authorization_endpoint: "ポータル認証エンドポイント"
  client_secret: "シークレット"
  client_id: "クライアントID"
  application_id: "アプリケーションID"
----
|===
+
* `**console_endpoint**` に、コンソールエンドポイントを指定します。 +
* `**portal_authorization_endpoint**` に、ポータル認証エンドポイントを指定します。 +
ポータル認証エンドポイントは、 `**\https://login.microsoftonline.com/{テナントID}**`  の形式で指定します。 +
* `**client_secret**` に、登録したアプリケーションのシークレットを指定します。 +
* `**client_id**` に、登録したアプリケーションのクライアントIDを指定します。 +
* `**application_id**` に、登録したアプリケーションのアプリケーションIDを指定します。 +
+

IMPORTANT: コンソールエンドポイントとクライアントIDとシークレットとテナントIDとアプリケーションIDの取得方法詳細は、「**Console**」の導入担当者(Service Administrator)に問い合わせください。 +
これらは「**Console**」へのアクセス情報となります。 +
公開したり、他者との共有をせず、取り扱いには十分注意してください。 +
+
NOTE: Proxy環境でサンプルアプリケーション実行する場合、環境変数 `**https_proxy**` の設定をしてください。

. Codespacesまたは、リポジトリをCloneした環境上で [src/common]配下に[azure_access_settings.yaml]を作成し接続先情報を設定します。 +
本設定は、推論結果取得先が「Azure Blob Storage」の時に設定します。


+
|===
|src/common/azure_access_settings.yaml
a|
[source,Yaml]
----
azure_access_settings:
  connection_string: "接続情報"
  container_name: "コンテナ名"
----
|===

* `**connection_string**` に、「Azure Blob Storage」の接続情報を指定します。 +
* `**container_name**` に、「Azure Blob Storage」のコンテナ名を指定します。
+

IMPORTANT: これらは「Azure Blob Storage」へのアクセス情報となります。 +
公開したり、他者との共有をせず、取り扱いには十分注意してください。 +

. Codespacesまたは、リポジトリをCloneした環境上で [src/common]配下に[get_client.py]に接続先情報を設定します。
+
|===
|src/common/get_client.py
a|
[source,Python]
----
class Service(Enum):
    Console = auto()
    Azure = auto()
    Local = auto()


CONNECTION_DESTINATION = Service.Console
LOCAL_ROOT = ""
----
|===
+
* `**CONNECTION_DESTINATION**` に、推論結果取得先を設定します。デフォルト値は `**Service.Console**` です。 +
* `**LOCAL_ROOT**` に、「Local Storage」のパスを指定します。 +
本設定は、 `**CONNECTION_DESTINATION**` に `**Service.Local**` を指定した場合に利用されます。 +

NOTE: Dev Container環境を利用する場合、Local Storageをgit cloneしたフォルダ内にフォルダ作成し、 +
`**LOCAL_ROOT**` は `**/workspace/{git cloneしたフォルダ内に作成したフォルダ}**` と設定する。

image::prepare_python_ja.png[alt="サンプルアプリケーション実行の準備をする",width="700",align="center"]

=== 2.サンプルアプリケーションを開始する
Codespacesまたは、リポジトリをCloneした環境上のターミナルからpackageのインストールとサンプルアプリケーションの起動を行います。

....
$ pip install .
$ python src/app.py
....

image::launch_app_python_ja.png[alt="サンプルアプリケーションを開始する",width="700",align="center"]

=== 3.推論を開始する
ブラウザからサンプルアプリケーションにアクセスして、各種操作を行います。

. ブラウザで http://localhost:3000 (Codespacesの場合は、ポート転送されたURL)を開く 
. [**DeviceID**]のリストからDevice IDを選択する
. [**START**]をクリックし、エッジAIデバイスの推論を開始する

image::start_inference_python_ja.png[alt="推論を開始する",width="700",align="center"]

=== 4.推論結果と画像を確認する
推論開始中は、"**Image/Inference**"エリアに画像と推論結果を表示します。

image::running_python_ja.png[alt="推論結果と画像を確認する",width="700",align="center"]


=== 5.推論を停止する
サンプルアプリケーションの[**STOP**]をクリックし、エッジAIデバイスの推論を停止します。

image::stop_inference_python_ja.png[alt="推論を停止する",width="700",align="center"]

== ユースケース毎の実装説明

=== 1.「**Console**」に登録されたエッジAIデバイスの情報を取得する

「**Console**」を利用するために、「**Cloud SDK**」のClientを生成します。 + 
生成したClientから、「**Console**」の提供する機能を利用します。


* ライブラリインポート
+

[source,Python]
----
from console_access_library.client import Client
from console_access_library.common.config import Config
----
上記のように、「**Cloud SDK**」のClient生成に必要なライブラリをimportします。


* 「**Cloud SDK**」のClient生成
+
[source,Python]
----
def get_console_client():
  config_obj = Config(
        read_console_access_settings_obj.console_endpoint,
        read_console_access_settings_obj.portal_authorization_endpoint,
        read_console_access_settings_obj.client_id,
        read_console_access_settings_obj.client_secret,
        read_console_access_settings_obj.application_id
    )
  client_obj = Client(config_obj)
  
  return client_obj
----
上記のソースコードで、「**Cloud SDK**」のClientを生成します。 +
`**Config**` に接続情報を指定し、 `**config_obj**` を生成します。 +
`**Client**` に `**config_obj**` を指定し、 `**client_obj**` を生成します。

* デバイス情報取得
+
[source,Python]
----
def get_devices():
  client_obj = get_console_client()
  client_obj.device_management.get_devices()
----
上記のように、`**get_console_client**` を呼び出し、 `**client_obj**` を生成します。 +
`**client_obj**` の `**device_management**` が提供する `**get_devices**` を使用してデバイスの情報を取得します。

* デバイスパラメータ取得
+
[source,Python]
----
def get_command_parameter_file():
  client_obj = get_client.get_console_client()
  return client_obj.device_management.get_command_parameter_file()
----
上記のように、`**get_console_client**` を呼び出し、 `**client_obj**` を生成します。 +
`**client_obj**` の `**device_management**` が提供する `**get_command_parameter_file**` を使用してデバイスのパラメータを取得します。


=== 2.エッジAIデバイスへ推論開始を指示する


* 推論開始
+
[source,Python]
----
def start_upload_inference_result():
  client_obj = get_console_client()
  return client_obj.device_management.start_upload_inference_result(device_id="device_id")
----
上記のように、`**get_console_client**` を呼び出し、 `**client_obj**` を生成します。 +
 `**client_obj**` の `**device_management**` が提供する `**start_upload_inference_result**` を使用して推論を開始します。

=== 3.「**Console**」の推論結果・画像を取得する

「**Console**」から推論結果・画像を取得するために、Clientが提供する機能を利用します。

* 画像リストを取得する
+
[source,Python]
----
def get_image_from_console(device_id, sub_directory, order_by=None, skip=None, number_of_images=None):
    """Get the image from Console
    Args:
        device_id (str): Device id
        sub_directory (str): image file uploaded directory
        order_by (str): Sort order by date and time the image was created. DESC, ASC
        skip (int): Number of images to skip fetching
        number_of_images (int): Number of images acquired
    Returns:
     total_image_count (int): get images number
        images :[{
            contents (str): base64 encode image data
            name (str): image file name
        }]
    """
    client_obj = get_client.get_console_client()

    image_response = client_obj.insight.get_image_data(device_id=device_id, sub_directory_name=sub_directory, number_of_images=number_of_images, skip=skip, order_by=order_by)

    return image_response
----
上記のように、`**get_console_client**` を呼び出し、 `**client_obj**` を生成します。 +
`**insight**` が提供する `**get_image_data**` を使用して画像リストを取得します。 +

* 最新の画像を取得し、推論結果と紐付ける
+
[source,Python]
----
  client_obj = get_client.get_console_client()
  image_response = client_obj.insight.get_image_data(device_id="device_id", sub_directory_name="sub_directory_name", number_of_images=1, skip=0, order_by="DESC")
  latest_image_data = "data:image/jpg;base64," + image_response["images"][0]["contents"]
  latest_image_ts = image_response["images"][0]["name"].replace(".jpg", "")
----
上記のソースコードで、画像のリストから最新の画像情報を取得します。 +
`**latest_image_data**` に、最新の画像データを取得します。 +
`**latest_image_ts**` に、最新の画像のタイムスタンプを取得します。 +
推論結果と画像はそれぞれのタイムスタンプで紐づいています。 +
`**latest_image_ts**` を使用して、画像に紐づいた推論結果の取得関数を呼び出します。

* 最新の画像に紐づく推論結果を取得する
+
[source,Python]
----
def get_inference_from_console(device_id, start_time=None, end_time=None, number_of_inference_result=None):
  """Get inference_data from Console
  Args:
    device_id (str): Device id
    start_time: Parameters used in filter options.
    end_time: Parameters used in filter options.
    number_of_inference_result: Parameters used in filter options.
  Returns:
    inference_response: inference result
  """
  client_obj = get_client.get_console_client()
  filter = f"EXISTS(SELECT VALUE i FROM i IN c.Inferences WHERE i.T >= \'{start_time}\' AND i.T <= \'{end_time}\')"
  raw = 1
  time = None
  inference_response = client_obj.insight.get_inference_results(device_id, filter, number_of_inference_results=number_of_inference_result, raw=raw, time=time)
  inferences_response = []
  for inference in inference_response:
    inferences_response.append(inference["inference_result"]["Inferences"][0]["O"])
  return inferences_response

----
上記のように、`**get_console_client**` を呼び出し、 `**client_obj**` を生成します。 +
`**insight**` が提供する `**get_inference_results**` を使用して推論結果のリストを取得します。 +
`**filter**` は検索フィルタを指定する引数です。 +
`**raw**` は格納された推論結果にアクセスするための引数です。 +
`**time**` は、取得する推論結果のタイムスタンプを指定します。

* 推論結果のDeserialize
+
[source,Python]
----
deserialize_data = get_deserialize_data.get_deserialize_data(latest_inference_data)
----
上記のソースコードでは、「**Console**」から取得した推論結果を参照可能な形式へ変換する処理を行っています。 +
この変換処理の詳細について、 https://github.com/SonySemiconductorSolutions/aitrios-sdk-deserialization-sample[「Cloud SDK Deserialize サンプル」] を参照してください。

=== 4.「Azure Blob Storage」の推論結果・画像を取得する
「Azure Blob Storage」から推論結果・画像を取得するために、storageディレクトリ配下のget_azure_storage.pyを利用します。

* 画像リストを取得する
+
[source,Python]
----
def get_image_from_azure(retry_count, device_id, sub_directory, order_by="ASC", skip=0, number_of_images=50):
  """Get the image from Azure
    Args:
      retry_count (int): Number of retries on failure,
      device_id (str): Device id
      sub_directory (str): image file uploaded directory
      order_by (str): Sort order by date and time the image was created. DESC, ASC
      skip (int): Number of images to skip fetching
      number_of_images (int): Number of images acquired
    Returns:
      total_image_count (int): get images number
      images :[{
        contents (str): base64 encode image data
        name (str): image file name
      }]
  """
  image_response = {
    "total_image_count": 0,
    "images": []
  }
  access_info = get_client.get_azure_access_settings()
  blob_service_client = BlobServiceClient.from_connection_string(
    access_info["connection_string"])
  container_client = blob_service_client.get_container_client(
    access_info["container_name"])

  storage_path = f"{device_id}/image/{sub_directory}"
  blobs_list = container_client.list_blobs(storage_path)
  blob_name_array = []
  for blob in blobs_list:
    blob_name_array.append(blob.name)
  if order_by == "DESC":
    blob_name_array.sort(reverse=True)

  images = []

  for i in range(skip, skip + number_of_images):
    if i == number_of_images:
      break
    if i > len(blob_name_array):
      break
    blob_client = container_client.get_blob_client(blob_name_array[i])
    image = blob_client.download_blob()
    image_text = image.read()
    b64encoded = base64.b64encode(image_text)

    file_name = blob_name_array[i].rsplit("/", 1)[1]
    images.append({
      "name": file_name,
      "contents": b64encoded.decode()
    })

  if len(blob_name_array) != 0:
    image_response["total_image_count"] = len(blob_name_array)
    image_response["images"] = images
    return image_response

  if retry_count > 0:
    time.sleep(1)
    return get_image_from_azure(retry_count - 1,
                                device_id,
                                sub_directory,
                                order_by,
                                skip,
                                number_of_images)

  return image_response

----
`**azure.storage.blob**` が提供する `**list_blobs**` を使用して画像ファイル名のリストを取得します。 +
`**azure.storage.blob**` が提供する `**get_blob_client**` 、 `**download_blob**` 、 `**read**` を利用して画像データを取得します。 +
画像ファイル名とbase64を作成し、`**total_image_count**` と合わせて返却します。

* 最新の画像に紐づく推論結果を取得する
+
[source,Python]
----
def get_inference_from_azure(retry_count, 
                            device_id, sub_directory,
                            start_inference_time=None,
                            end_inference_time=None,
                            number_of_inference_result=None):
  """Get inference_data from Azure
    Args:
      retry_count (int): Number of retries on failure
      device_id (str): Device id
      sub_directory (str): image file uploaded directory
      start_inference_time (str): start range.
      end_inference_time (str): end range.
      number_of_inference_result (int): Number of cases to get.
    Returns:
      inferences (arr): inference results
  """
  inferences = []
  access_info = get_client.get_azure_access_settings()
  blob_service_client = BlobServiceClient.from_connection_string(access_info["connection_string"])
  container_client = blob_service_client.get_container_client(access_info["container_name"])
  storage_path = os.path.join(device_id, "metadata", sub_directory)
  blobs = []

  for blob in container_client.list_blobs(storage_path):
    timestamp = blob.name.split("/")[3].replace(".txt", "")
    if (start_inference_time is None or timestamp >= start_inference_time) and\
        (end_inference_time is None or timestamp < end_inference_time):
        blobs.append(blob.name)
    if end_inference_time is not None and timestamp > end_inference_time:
      break

    if len(blobs) == number_of_inference_result:
      break

  if len(blobs) != 0:
    for blob_name in blobs:
      blob_client = container_client.get_blob_client(blob_name)
      inference = blob_client.download_blob(encoding="UTF-8")
      inference_text = inference.readall()
      inference_obj = json.loads(inference_text)
      inferences.append(inference_obj["Inferences"][0]["O"])
    return inferences

  if retry_count > 0:
    time.sleep(1)
    return get_inference_from_azure(retry_count - 1,
                                    device_id,
                                    sub_directory,
                                    start_inference_time,
                                    end_inference_time,
                                    number_of_inference_result)
  return inferences

----
`**azure.storage.blob**` が提供する `**list_blobs**` を使用して推論結果ファイル名のリストを取得します。 +
取得した推論結果ファイル名のタイムスタンプが指定範囲内か確認します。 +
`**azure.storage.blob**` が提供する `**get_blob_client**` 、 `**download_blob**` 、 `**readall**` を利用して推論結果データを取得します。
`**start_inference_time**` は検索開始位置を表すタイムスタンプです。 +
`**end_inference_time**` は検索終了位置を表すタイムスタンプです。 +
`**number_of_inference_result**` は取得する推論結果の数です。 +

=== 5.「Local Storage」の推論結果・画像を取得する
「Local Storage」から推論結果・画像を取得するために、storageディレクトリ配下のget_local_storage.pyを利用します。

* 画像リストを取得する
+
[source,Python]
----
def get_image_from_local(device_id, sub_directory, order_by="ASC", skip=0, number_of_images=50):
  """Get the image from Local
  Args:
    device_id (str): Device id
    sub_directory (str): image file uploaded directory
    order_by (str): Sort order by date and time the image was created. DESC, ASC
    skip (int): Number of images to skip fetching
    number_of_images (int): Number of images acquired
  Returns:
    total_image_count (int): get images number
      images :[{
        contents (str): base64 encode image data
        name (str): image file name
      }]
  """
  storage_path = os.path.join(get_client.LOCAL_ROOT, device_id, "image", sub_directory)
  order_by = order_by.upper() if order_by else "ASC"
  images = []
  if not os.path.exists(storage_path):
    raise FileNotFoundError("Only absolute paths are supported.")

  files = os.listdir(storage_path)
  image_files = []
  for file in files:
    if file.lower().endswith(".jpg"):
      image_files.append(file)

  if order_by == "DESC":
    image_files.reverse()

  for i, image_file in enumerate(image_files[skip:]):
    if i == number_of_images:
      break

    file_path = os.path.join(storage_path, image_file)
    symbolic_link = Path(file_path).is_symlink()
    if symbolic_link is True:
      raise IsADirectoryError("Can't open symbolic link file.")
    with open(file_path, "rb") as file:
      data = file.read()
      base64_image = base64.b64encode(data).decode("utf-8")
      images.append({
        "name": image_file,
        "contents": base64_image
      })

  response = {
    "total_image_count": len(image_files),
    "images": images
  }

  return response

----
`**os**` が提供する `**listdir**` を使用して画像ファイル名のリストを取得します。 +
`**open**` を利用して画像データを取得します。 +
画像ファイル名とbase64を作成し、`**total_image_count**` と合わせて返却します。

* 最新の画像に紐づく推論結果を取得する
+
[source,Python]
----
def get_inference_from_local(device_id, sub_directory, start_inference_time=None, end_inference_time=None, number_of_inference_result=20):
  """Get inference_data from Local
  Args:
      device_id (str): Device id
      sub_directory (str): image file uploaded directory
      start_inference_time (str): When this value is specified, extract the inference result metadata within the following range.
      end_inference_time (str): When this value is specified, extract the inference result metadata within the following range.
      number_of_inference_result (int): Number of cases to get.
  Returns:
      inferences (arr): inference results
  """
  storage_path = os.path.join(get_client.LOCAL_ROOT, device_id, "meta", sub_directory)
  inference_results = []
  if not os.path.exists(storage_path):
    raise FileNotFoundError("Data does not exist.")

  inference_files = os.listdir(storage_path)
  for file_name in inference_files:
    timestamp = os.path.splitext(file_name)[0]
    if (start_inference_time is None or timestamp >= start_inference_time) and\
      (end_inference_time is None or timestamp < end_inference_time):
      inference_data_path = os.path.join(storage_path, file_name)
      symbolic_link = Path(inference_data_path).is_symlink()
      if symbolic_link is True:
        raise IsADirectoryError("Can't open symbolic link file.")
      with open(inference_data_path, "r") as file:
        inference_data = json.load(file)
        inference_result = inference_data["Inferences"][0]["O"]
        inference_results.append(inference_result)
    elif end_inference_time is not None and timestamp > end_inference_time:
      break
    if len(inference_results) == number_of_inference_result:
      break

  return inference_results
----
`**os**` が提供する `**listdir**` を使用して推論結果ファイル名のリストを取得します。 +
取得した推論結果ファイル名のタイムスタンプが指定範囲内か確認します。 +
`**open**` を利用して推論結果データを取得します。 +
`**start_inference_time**` は検索開始位置を表すタイムスタンプです。 +
`**end_inference_time**` は検索終了位置を表すタイムスタンプです。 +
`**number_of_inference_result**` は取得する推論結果の数です。 +

=== 6.エッジAIデバイスへの推論停止を指示する

* 推論停止
+
[source,Python]
----
def stop_upload_inference_result():
  client_obj = get_console_client()
  return client_obj.device_management.stop_upload_inference_result(device_id="device_id")
----
エッジAIデバイスの推論処理を停止するには、上記のように `**client_obj**` の `**device_management**` が提供する `**stop_upload_inference_result**` を実行します。 +
引数の `**device_id**` には、停止対象の Device ID を指定します。

== 参考資料

=== 取得した推論結果の表示（サンプルアプリケーションの表示処理）

[source,JavaScript]
----
function drawBoundingBox (image, inferenceData, labeldata) {
  const img = new window.Image()
  img.src = image
  img.onload = () => {
    const canvas = document.getElementById('canvas')
    const canvasContext = canvas.getContext('2d')
    canvas.width = img.width
    canvas.height = img.height
    canvasContext.drawImage(img, 0, 0)

    // 取得した推論結果を表示
    for (const [key, value] of Object.entries(inferenceData)) {
      if (key === 'T') {
        continue
      }
      canvasContext.lineWidth = 3
      canvasContext.strokeStyle = 'rgb(255, 255, 0)'
      // バウンディングボックスの座標を指定
      canvasContext.strokeRect(value.left, value.top, Math.abs(value.left - value.right), Math.abs(value.bottom - value.top))
      canvasContext.font = '20px Arial'
      canvasContext.fillStyle = 'rgba(255, 255, 0)'

      // ラベルを表示する座標を指定
      const labelPointX = (value.right > 270 ? value.right - 70 : value.right)
      const labelPointY = (value.bottom > 300 ? value.bottom - 10 : value.bottom)

      // ラベル、確率を表示
      canvasContext.fillText(labeldata[value.class_id] + ' ' + Math.round((value.score) * 100) + '%', labelPointX, labelPointY)
    }
  }
}
----

* 画像リストで取得される画像パスのフォーマット
+
----
<blobcontainer_name>/<device_id>/JPG/<sub_directory_name>/YYYYMMDDHHMMSSFFF.jpg
----
* 推論結果（Object Detection）のサンプルデータ + 
Inferences[]の部分が推論結果 + 
下記サンプルデータでは、2件のオブジェクト検出 +
検出結果はserializeされているが、下記サンプルデータではdeserializeされたデータ形式
+
[source,Json]
----
{
    "DeviceID": "123456789ABC",
    "ModelID": "0000000000000000",
    "Image": true,
    "Inferences": [
        {
            "1": {
                "class_id": 18,
                "score": 0.03125,
                "left": 8,
                "top": 0,
                "right": 303,
                "bottom": 107
            },
            "2": {
                "class_id": 19,
                "score": 0.02734375,
                "left": 2,
                "top": 230,
                "right": 38,
                "bottom": 319
            },
            "T": "20220101010101000"
        }
    ],
    "id": "00000000-0000-0000-0000-000000000000",
    "_rid": "AAAAAAAAAAAAAAAAAAAAAA==",
    "_self": "dbs/XXXXXX==/colls/CCCCCCCCCCCC=/docs/AAAAAAAAAAAAAAAAAAAAAA==/",
    "_etag": "\"00000000-0000-0000-0000-000000000000\"",
    "_attachments": "attachments/",
    "_ts": 1111111111
}
----
+
検出結果のパラメータは下記の通りです。
+
class_id: オブジェクトラベルのindex
+
score: オブジェクトラベルの確度
+
left: オブジェクトのX座標開始位置
+
top: オブジェクトのY座標開始位置
+
right: オブジェクトのX座標終了位置
+
bottom: オブジェクトのY座標終了位置
